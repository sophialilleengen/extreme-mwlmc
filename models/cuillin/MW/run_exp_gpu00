#!/bin/bash
#SBATCH --job-name=MW0 	                   # set job name
#SBATCH --mail-user=michael.petersen@roe.ac.uk # set your mail address
#SBATCH -p GPU                             # set partition (queue)
#SBATCH --nodes=1                          # set node number
#SBATCH --gres=gpu:4                       # set GPU number
#SBATCH --ntasks-per-gpu=1                         # set task number
#SBATCH --cpus-per-task=8
#SBATCH --mem=128G                          # set memory request sbatch: fatal: --mem, --mem-per-cpu, and --mem-per-gpu are mutually exclusive.
#SBATCH --time=71:00:00                        # set max wallclock time
#SBATCH --output=job-%N-%j.out                 # set output log name
#SBATCH --error=job-%N-%j.error                # set error log name %N=first node, %j=job number
#SBATCH --mail-type=ALL                        # set mail alerts

# set up multithreading
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# load modules
module purge
module load cuda hdf5-openmpi

# append a new set of libraries to point at EXP executable:
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/mpetersen/lib:/home/mpetersen/local/lib:/home/mpetersen/lib

# change directory to proper location
cd /cephfs/mpetersen/extreme-mwlmc/models/cuillin/MW

# stamp output with details
pwd; hostname; date

# print the current path to libraries:
echo $LD_LIBRARY_PATH


# set the execution command
export COMMAND="mpirun -v /home/mpetersen/bin/exp run_mw_model_gpu00.yml"

# stamp output with execution command
echo $COMMAND

# run execution command
$COMMAND

# here, you could stack up multiple execution commands if you wanted. they will run serially.

# stamp output with end time
date

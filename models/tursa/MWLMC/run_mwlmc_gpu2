#!/bin/bash
#SBATCH --job-name=MWLMC                          # set job name
#SBATCH --mail-user=michael.petersen@roe.ac.uk # set your mail address
#SBATCH -p gpu                             # set partition (queue)
#SBATCH --qos=standard
#SBATCH --nodes=2                          # set node number
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4                       # set GPU number per node
#SBATCH --cpus-per-task=1
#SBATCH --mem=384G
#SBATCH --time=47:00:00                        # set max wallclock time
#SBATCH --output=job-%N-%j.out                 # set output log name
#SBATCH --error=job-%N-%j.error                # set error log name %N=first node, %j=job number
#SBATCH --mail-type=ALL                        # set mail alerts

# budgeting
#SBATCH --account=dp309

# prepend a new set of libraries to point at EXP executable:
export LD_LIBRARY_PATH=/home/dp309/dp309/dc-pete4/lib:$LD_LIBRARY_PATH

# change directory to proper location
cd /home/dp309/dp309/shared/extreme-mwlmc/models/tursa/MWLMC/

# stamp output with details
pwd; hostname; date


# set up multithreading
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OMP_PLACES=cores

# load modules
module load /home/y07/shared/tursa-modules/setup-env
module load cmake
module load gcc/9.3.0
module load cuda/12.3
module load openmpi/4.1.5-cuda12.3 

# set the compute capability for the GPUs
export CUDAARCHS=80

# see https://epcced.github.io/dirac-docs/tursa-user-guide/scheduler/#example-job-submission-scripts
# recommended MPI/OMP settings


# These will need to be changed to match the actual application you are running
application="/home/dp309/dp309/dc-pete4/bin/exp"
options="run_mwlmc_model_gpu4.yml"

# stamp output with execution command
echo ${application} ${options}

# We have reserved the full nodes, now distribute the processes as
# required: 4 MPI processes per node, stride of 8 cores between 
# MPI processes
# 
# Note use of gpu_launch.sh wrapper script for GPU and NIC pinning 
srun --nodes=2 --ntasks-per-node=4 --cpus-per-task=1 \
     --hint=nomultithread --distribution=block:block \
     gpu_launch.sh \
     ${application} ${options}

# stamp output with end time
date
